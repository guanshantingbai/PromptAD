#!/usr/bin/env python3
"""
6ç±»å¯¹ç…§å®éªŒæ±‡æ€»åˆ†æ
å¯¹æ¯” Baseline vs Prompt2 vs Ours

è¾“å‡ºï¼š
1. æ€§èƒ½å¯¹æ¯”è¡¨ï¼ˆAUROCï¼‰
2. Margin/Separationå¯¹æ¯”è¡¨
3. CollapseæŒ‡æ ‡å¯¹æ¯”
4. å®šæ€§ç»“è®º
"""

import pandas as pd
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 200)


# å®šä¹‰6ä¸ªä»£è¡¨æ€§ç±»åˆ«
CLASS_INFO = {
    'mvtec-toothbrush': 'Severe',
    'mvtec-capsule': 'Severe',
    'visa-pcb2': 'Severe',
    'mvtec-carpet': 'Stable',
    'mvtec-leather': 'Stable',
    'mvtec-screw': 'Improved',
}


def load_split_auroc(class_key, version):
    """åŠ è½½æ‹†åˆ†AUROCç»“æœ"""
    dataset, cls = class_key.split('-')
    file_path = f'analysis/6class_comparison/{dataset}_{cls}_{version}_split_auroc.csv'
    
    if not Path(file_path).exists():
        return None
    
    df = pd.read_csv(file_path)
    return {
        'overall_semantic': df['overall_semantic_auroc'].values[0],
        'overall_fusion': df['overall_fusion_auroc'].values[0],
        'normal_semantic': df['normal_semantic_auroc'].values[0],
        'abnormal_semantic': df['abnormal_semantic_auroc'].values[0],
    }


def load_margin_stats(class_key, version):
    """åŠ è½½Marginç»Ÿè®¡"""
    dataset, cls = class_key.split('-')
    file_path = f'analysis/6class_comparison/{dataset}_{cls}_{version}_margin_stats.csv'
    
    if not Path(file_path).exists():
        return None
    
    df = pd.read_csv(file_path)
    
    # æå–normalå’Œabnormalç»„çš„ç»Ÿè®¡
    normal_row = df[df['group'] == 'normal']
    abnormal_row = df[df['group'] == 'abnormal']
    
    if len(normal_row) == 0 or len(abnormal_row) == 0:
        return None
    
    return {
        'normal_margin_mean': normal_row['mean'].values[0],
        'normal_margin_p10': normal_row['p10'].values[0],
        'abnormal_margin_mean': abnormal_row['mean'].values[0],
        'abnormal_margin_p90': abnormal_row['p90'].values[0],
        'separation': normal_row['mean'].values[0] - abnormal_row['mean'].values[0],
    }


def calculate_collapse_from_samples(class_key, version):
    """ä»æ ·æœ¬åˆ†æ•°è®¡ç®—collapseæŒ‡æ ‡ï¼ˆè¿‘ä¼¼ï¼‰"""
    dataset, cls = class_key.split('-')
    file_path = f'analysis/6class_comparison/{dataset}_{cls}_{version}_sample_scores.csv'
    
    if not Path(file_path).exists():
        return None
    
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šä»æ ·æœ¬åˆ†æ•°çš„æ–¹å·®æ¨æµ‹collapseç¨‹åº¦
    # å®é™…collapseéœ€è¦ä»åŸå‹ç›¸ä¼¼åº¦è®¡ç®—ï¼Œè¿™é‡Œç”¨æ ·æœ¬åˆ†æ•°æ–¹å·®ä½œä¸ºä»£ç†
    df = pd.read_csv(file_path)
    
    # Collapseä»£ç†ï¼šsemantic_scoreçš„æ ‡å‡†å·®ï¼ˆä½æ–¹å·®è¡¨ç¤ºé«˜collapseï¼‰
    semantic_std = df['semantic_score'].std()
    
    return {
        'semantic_score_std': semantic_std,
    }


def aggregate_comparison():
    """æ±‡æ€»ä¸‰ç‰ˆæœ¬å¯¹æ¯”"""
    print("="*80)
    print("6ç±»ä»£è¡¨æ€§ç±»åˆ« - ä¸‰ç‰ˆæœ¬å¯¹æ¯”æ±‡æ€»")
    print("="*80)
    print()
    
    results = []
    
    for class_key, group in CLASS_INFO.items():
        print(f"å¤„ç† {class_key} ({group})...")
        
        # åŠ è½½ä¸‰ä¸ªç‰ˆæœ¬çš„æ•°æ®
        baseline_auroc = load_split_auroc(class_key, 'baseline')
        prompt2_auroc = load_split_auroc(class_key, 'prompt2')
        ours_auroc = load_split_auroc(class_key, 'ours')
        
        baseline_margin = load_margin_stats(class_key, 'baseline')
        prompt2_margin = load_margin_stats(class_key, 'prompt2')
        ours_margin = load_margin_stats(class_key, 'ours')
        
        baseline_collapse = calculate_collapse_from_samples(class_key, 'baseline')
        prompt2_collapse = calculate_collapse_from_samples(class_key, 'prompt2')
        ours_collapse = calculate_collapse_from_samples(class_key, 'ours')
        
        # æ„é€ ç»“æœè¡Œ
        result = {
            'class': class_key,
            'group': group,
        }
        
        # AUROCå¯¹æ¯”
        if baseline_auroc and prompt2_auroc and ours_auroc:
            result['baseline_auroc'] = baseline_auroc['overall_semantic']
            result['prompt2_auroc'] = prompt2_auroc['overall_semantic']
            result['ours_auroc'] = ours_auroc['overall_semantic']
            result['delta_prompt2'] = prompt2_auroc['overall_semantic'] - baseline_auroc['overall_semantic']
            result['delta_ours'] = ours_auroc['overall_semantic'] - baseline_auroc['overall_semantic']
            result['improvement_vs_prompt2'] = ours_auroc['overall_semantic'] - prompt2_auroc['overall_semantic']
        
        # Marginå¯¹æ¯”
        if baseline_margin and prompt2_margin and ours_margin:
            result['baseline_separation'] = baseline_margin['separation']
            result['prompt2_separation'] = prompt2_margin['separation']
            result['ours_separation'] = ours_margin['separation']
            result['separation_change'] = ours_margin['separation'] - prompt2_margin['separation']
            
            result['baseline_normal_margin'] = baseline_margin['normal_margin_mean']
            result['prompt2_normal_margin'] = prompt2_margin['normal_margin_mean']
            result['ours_normal_margin'] = ours_margin['normal_margin_mean']
        
        # Collapseå¯¹æ¯”ï¼ˆä»£ç†æŒ‡æ ‡ï¼‰
        if baseline_collapse and prompt2_collapse and ours_collapse:
            result['baseline_semantic_std'] = baseline_collapse['semantic_score_std']
            result['prompt2_semantic_std'] = prompt2_collapse['semantic_score_std']
            result['ours_semantic_std'] = ours_collapse['semantic_score_std']
        
        results.append(result)
    
    # è½¬ä¸ºDataFrame
    df = pd.DataFrame(results)
    
    # ä¿å­˜è¯¦ç»†å¯¹æ¯”è¡¨
    output_path = 'analysis/6class_comparison/comparison_summary.csv'
    df.to_csv(output_path, index=False, float_format='%.4f')
    print(f"\nâœ… è¯¦ç»†å¯¹æ¯”è¡¨å·²ä¿å­˜: {output_path}")
    
    return df


def print_summary_analysis(df):
    """æ‰“å°æ±‡æ€»åˆ†æ"""
    print("\n" + "="*80)
    print("ğŸ“Š å…³é”®æŒ‡æ ‡æ±‡æ€»")
    print("="*80)
    
    # æŒ‰ç»„ç»Ÿè®¡
    for group in ['Severe', 'Stable', 'Improved']:
        group_df = df[df['group'] == group]
        if len(group_df) == 0:
            continue
        
        print(f"\nã€{group}ç»„ã€‘(n={len(group_df)})")
        
        # AUROCå˜åŒ–
        if 'delta_prompt2' in group_df.columns:
            print(f"  AUROCå˜åŒ–:")
            print(f"    Baselineâ†’Prompt2: {group_df['delta_prompt2'].mean():+.4f} (å¹³å‡)")
            print(f"    Baselineâ†’Ours:    {group_df['delta_ours'].mean():+.4f} (å¹³å‡)")
            print(f"    Prompt2â†’Ours:     {group_df['improvement_vs_prompt2'].mean():+.4f} (å¹³å‡)")
        
        # Separationå˜åŒ–
        if 'separation_change' in group_df.columns:
            print(f"  Separationå˜åŒ–:")
            print(f"    Prompt2: {group_df['prompt2_separation'].mean():.4f}")
            print(f"    Ours:    {group_df['ours_separation'].mean():.4f}")
            print(f"    å˜åŒ–:    {group_df['separation_change'].mean():+.4f}")
    
    # æ•´ä½“ç»Ÿè®¡
    print(f"\nã€æ•´ä½“ã€‘(n={len(df)})")
    
    if 'improvement_vs_prompt2' in df.columns:
        improvement_count = (df['improvement_vs_prompt2'] > 0).sum()
        print(f"  Oursç›¸å¯¹Prompt2:")
        print(f"    æ”¹å–„ç±»åˆ«æ•°: {improvement_count}/{len(df)}")
        print(f"    å¹³å‡AUROCæå‡: {df['improvement_vs_prompt2'].mean():+.4f}")
        print(f"    æœ€å¤§æå‡: {df['improvement_vs_prompt2'].max():+.4f} ({df.loc[df['improvement_vs_prompt2'].idxmax(), 'class']})")
        print(f"    æœ€å¤§ä¸‹é™: {df['improvement_vs_prompt2'].min():+.4f} ({df.loc[df['improvement_vs_prompt2'].idxmin(), 'class']})")
    
    if 'separation_change' in df.columns:
        separation_improve = (df['separation_change'] > 0).sum()
        print(f"\n  Separationæ”¹å–„:")
        print(f"    æ”¹å–„ç±»åˆ«æ•°: {separation_improve}/{len(df)}")
        print(f"    å¹³å‡å˜åŒ–: {df['separation_change'].mean():+.4f}")


def generate_qualitative_conclusions(df):
    """ç”Ÿæˆå®šæ€§ç»“è®º"""
    print("\n" + "="*80)
    print("ğŸ’¡ å®šæ€§ç»“è®º")
    print("="*80)
    
    # 1. Severeç»„é€€åŒ–æ˜¯å¦ç¼“è§£ï¼Ÿ
    severe_df = df[df['group'] == 'Severe']
    if len(severe_df) > 0 and 'improvement_vs_prompt2' in severe_df.columns:
        severe_improvement = severe_df['improvement_vs_prompt2'].mean()
        if severe_improvement > 0.02:
            print(f"\nâœ… è¯æ®å……åˆ†: Severeç»„é€€åŒ–æ˜¾è‘—ç¼“è§£")
            print(f"   å¹³å‡AUROCæå‡: {severe_improvement:+.4f}")
            print(f"   â†’ ä¸‰é¡¹æ”¹åŠ¨å¯¹ä¸¥é‡é€€åŒ–ç±»åˆ«æœ‰æ•ˆ")
        elif severe_improvement > 0:
            print(f"\nâš–ï¸ è¶‹åŠ¿: Severeç»„ç•¥æœ‰æ”¹å–„")
            print(f"   å¹³å‡AUROCæå‡: {severe_improvement:+.4f}")
        else:
            print(f"\nâŒ Severeç»„æœªæ˜æ˜¾æ”¹å–„")
            print(f"   å¹³å‡AUROCå˜åŒ–: {severe_improvement:+.4f}")
    
    # 2. Margin/Separationæ˜¯å¦æ”¹å–„ï¼Ÿ
    if 'separation_change' in df.columns:
        avg_sep_change = df['separation_change'].mean()
        if avg_sep_change > 0.05:
            print(f"\nâœ… è¯æ®å……åˆ†: Separationæ˜¾è‘—æå‡")
            print(f"   å¹³å‡å˜åŒ–: {avg_sep_change:+.4f}")
            print(f"   â†’ Margin lossæœ‰æ•ˆæ‰©å¤§åˆ¤åˆ«è£•åº¦")
        elif avg_sep_change > 0:
            print(f"\nâš–ï¸ è¶‹åŠ¿: Separationç•¥æœ‰æå‡")
            print(f"   å¹³å‡å˜åŒ–: {avg_sep_change:+.4f}")
    
    # 3. Screwæ˜¯å¦ä¿æŒæ”¹è¿›ï¼Ÿ
    screw_df = df[df['class'] == 'mvtec-screw']
    if len(screw_df) > 0 and 'improvement_vs_prompt2' in screw_df.columns:
        screw_change = screw_df['improvement_vs_prompt2'].values[0]
        if screw_change >= -0.02:
            print(f"\nâœ… Screwä¿æŒæ”¹è¿›æˆ–è½»å¾®å›é€€")
            print(f"   ç›¸å¯¹Prompt2å˜åŒ–: {screw_change:+.4f}")
            print(f"   â†’ æ”¹åŠ¨æœªç ´åå›°éš¾ç±»çš„æå‡")
        else:
            print(f"\nâš ï¸ Screwæ˜¾è‘—å›é€€")
            print(f"   ç›¸å¯¹Prompt2å˜åŒ–: {screw_change:+.4f}")
    
    # 4. ä¸»è¦æ”¹å–„åœ¨å“ªä¸€ä¾§ï¼Ÿ
    # è¿™éœ€è¦ä»split AUROCæ•°æ®æ¨æ–­ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
    if 'prompt2_normal_margin' in df.columns and 'ours_normal_margin' in df.columns:
        normal_margin_change = (df['ours_normal_margin'] - df['prompt2_normal_margin']).mean()
        if normal_margin_change > 0.05:
            print(f"\nâœ… è¯æ®å……åˆ†: ä¸»è¦æ”¹å–„åœ¨Normalä¾§")
            print(f"   Normal marginå¹³å‡æå‡: {normal_margin_change:+.4f}")
            print(f"   â†’ å‡å°‘å‡é˜³æ€§ï¼ˆæ­£å¸¸æ ·æœ¬è¢«è¯¯åˆ¤ï¼‰")


def plot_comparison_charts(df):
    """ç”Ÿæˆå¯¹æ¯”å›¾è¡¨"""
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. AUROCå¯¹æ¯”ï¼ˆä¸‰ç‰ˆæœ¬ï¼‰
    ax = axes[0, 0]
    if 'baseline_auroc' in df.columns:
        x = np.arange(len(df))
        width = 0.25
        ax.bar(x - width, df['baseline_auroc'], width, label='Baseline', alpha=0.8)
        ax.bar(x, df['prompt2_auroc'], width, label='Prompt2', alpha=0.8)
        ax.bar(x + width, df['ours_auroc'], width, label='Ours', alpha=0.8)
        ax.set_xlabel('Class')
        ax.set_ylabel('AUROC')
        ax.set_title('AUROC Comparison (3 Versions)')
        ax.set_xticks(x)
        ax.set_xticklabels([c.split('-')[1] for c in df['class']], rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
    
    # 2. AUROCå˜åŒ–ï¼ˆOurs vs Prompt2ï¼‰
    ax = axes[0, 1]
    if 'improvement_vs_prompt2' in df.columns:
        colors = ['green' if x > 0 else 'red' for x in df['improvement_vs_prompt2']]
        ax.barh(df['class'], df['improvement_vs_prompt2'], color=colors, alpha=0.7)
        ax.axvline(x=0, color='black', linestyle='--', linewidth=0.8)
        ax.set_xlabel('AUROC Change (Ours - Prompt2)')
        ax.set_title('Performance Improvement')
        ax.grid(True, alpha=0.3, axis='x')
    
    # 3. Separationå¯¹æ¯”
    ax = axes[1, 0]
    if 'prompt2_separation' in df.columns:
        x = np.arange(len(df))
        width = 0.35
        ax.bar(x - width/2, df['prompt2_separation'], width, label='Prompt2', alpha=0.8)
        ax.bar(x + width/2, df['ours_separation'], width, label='Ours', alpha=0.8)
        ax.set_xlabel('Class')
        ax.set_ylabel('Separation (Normal - Abnormal Margin)')
        ax.set_title('Margin Separation Comparison')
        ax.set_xticks(x)
        ax.set_xticklabels([c.split('-')[1] for c in df['class']], rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
    
    # 4. æŒ‰ç»„æ±‡æ€»
    ax = axes[1, 1]
    if 'improvement_vs_prompt2' in df.columns:
        group_means = df.groupby('group')['improvement_vs_prompt2'].mean()
        colors_map = {'Severe': 'red', 'Stable': 'green', 'Improved': 'blue'}
        colors = [colors_map.get(g, 'gray') for g in group_means.index]
        ax.bar(group_means.index, group_means.values, color=colors, alpha=0.7)
        ax.axhline(y=0, color='black', linestyle='--', linewidth=0.8)
        ax.set_ylabel('Avg AUROC Change (Ours - Prompt2)')
        ax.set_title('Improvement by Performance Group')
        ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    output_path = 'analysis/6class_comparison/comparison_charts.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"\nâœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜: {output_path}")
    plt.close()


def main():
    # 1. æ±‡æ€»æ•°æ®
    df = aggregate_comparison()
    
    if len(df) == 0:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ evaluate_6class_comparison.sh")
        return
    
    # 2. æ‰“å°æ±‡æ€»åˆ†æ
    print_summary_analysis(df)
    
    # 3. ç”Ÿæˆå®šæ€§ç»“è®º
    generate_qualitative_conclusions(df)
    
    # 4. ç”Ÿæˆå›¾è¡¨
    plot_comparison_charts(df)
    
    print("\n" + "="*80)
    print("âœ… 6ç±»å¯¹ç…§å®éªŒæ±‡æ€»å®Œæˆï¼")
    print("="*80)
    print("\nå…³é”®æ–‡ä»¶:")
    print("  - analysis/6class_comparison/comparison_summary.csv")
    print("  - analysis/6class_comparison/comparison_charts.png")


if __name__ == '__main__':
    main()
