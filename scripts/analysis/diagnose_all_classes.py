#!/usr/bin/env python3
"""
Step 2: æ‰¹é‡è¿è¡Œè¯Šæ–­è„šæœ¬ï¼Œæ±‡æ€»å…¨éƒ¨27ä¸ªç±»åˆ«çš„æŒ‡æ ‡
"""

import os
import subprocess
import json
import pandas as pd
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from datetime import datetime

# è¯»å–ç±»åˆ«åˆ—è¡¨
performance_data = pd.read_csv('analysis/full_performance_comparison_k2.csv')
classes_to_diagnose = performance_data['class'].tolist()

def run_diagnosis(class_name):
    """è¿è¡Œå•ä¸ªç±»åˆ«çš„è¯Šæ–­"""
    try:
        cmd = [
            'python', 'diagnose_prototypes.py',
            '--k-shot', '2',
            '--classes', class_name
        ]
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode == 0:
            return {'class': class_name, 'status': 'success'}
        else:
            return {'class': class_name, 'status': 'failed', 'error': result.stderr[-200:]}
    except subprocess.TimeoutExpired:
        return {'class': class_name, 'status': 'timeout'}
    except Exception as e:
        return {'class': class_name, 'status': 'error', 'error': str(e)}

def main():
    print("="*80)
    print("Step 2: æ‰¹é‡è¯Šæ–­å…¨éƒ¨27ä¸ªç±»åˆ«")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ€»ç±»åˆ«æ•°: {len(classes_to_diagnose)}")
    print(f"å¹¶è¡Œè¿›ç¨‹æ•°: 4")
    print("="*80)
    print()
    
    # æ£€æŸ¥å·²å®Œæˆçš„ç±»åˆ«
    existing_diagnostics = []
    for class_name in classes_to_diagnose:
        dataset = class_name.split('-')[0]
        cls_short = class_name.split('-')[1]
        summary_path = f'diagnostics/{dataset}_k2_{cls_short}/summary.json'
        if os.path.exists(summary_path):
            existing_diagnostics.append(class_name)
    
    print(f"âœ… å·²å®Œæˆè¯Šæ–­: {len(existing_diagnostics)}/{len(classes_to_diagnose)} ä¸ªç±»åˆ«")
    
    # éœ€è¦è¿è¡Œçš„ç±»åˆ«
    classes_to_run = [c for c in classes_to_diagnose if c not in existing_diagnostics]
    
    if len(classes_to_run) == 0:
        print("æ‰€æœ‰ç±»åˆ«å·²å®Œæˆè¯Šæ–­ï¼Œè·³è¿‡æ‰¹é‡è¿è¡Œ")
    else:
        print(f"â³ éœ€è¦è¿è¡Œ: {len(classes_to_run)} ä¸ªç±»åˆ«")
        print()
        
        # å¹¶è¡Œè¿è¡Œè¯Šæ–­
        results = []
        with ProcessPoolExecutor(max_workers=4) as executor:
            futures = {executor.submit(run_diagnosis, cls): cls for cls in classes_to_run}
            
            completed = 0
            for future in as_completed(futures):
                result = future.result()
                results.append(result)
                completed += 1
                
                status_icon = "âœ…" if result['status'] == 'success' else "âŒ"
                print(f"[{completed}/{len(classes_to_run)}] {status_icon} {result['class']}: {result['status']}")
        
        print()
        print("="*80)
        print(f"æ‰¹é‡è¯Šæ–­å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r['status'] == 'success')
        failed_count = len(results) - success_count
        print(f"æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
        
        if failed_count > 0:
            print("\nå¤±è´¥çš„ç±»åˆ«:")
            for r in results:
                if r['status'] != 'success':
                    print(f"  - {r['class']}: {r['status']}")
                    if 'error' in r:
                        print(f"    é”™è¯¯: {r['error']}")
    
    print()
    print("="*80)
    print("æ±‡æ€»è¯Šæ–­æŒ‡æ ‡...")
    print("="*80)
    
    # æ±‡æ€»æ‰€æœ‰ç±»åˆ«çš„è¯Šæ–­ç»“æœ
    all_metrics = []
    missing_classes = []
    
    for class_name in classes_to_diagnose:
        dataset = class_name.split('-')[0]
        cls_short = class_name.split('-')[1]
        summary_path = f'diagnostics/{dataset}_k2_{cls_short}/summary.json'
        
        if os.path.exists(summary_path):
            with open(summary_path, 'r') as f:
                summary = json.load(f)
                
                # æå–å…³é”®æŒ‡æ ‡
                metrics = {
                    'class': class_name,
                    'dataset': dataset,
                    # Metric A: å¼‚å¸¸maxå¶ç„¶å‘½ä¸­
                    'A_hit_mean': summary['metric_A']['mean'],
                    'A_hit_p95': summary['metric_A']['p95'],
                    'A_hit_rate_30': summary['metric_A']['hit_rate_0.3'],
                    # Metric B: åˆ¤åˆ«è£•åº¦
                    'B_normal_margin': summary['metric_B']['normal_mean'],
                    'B_abnormal_margin': summary['metric_B']['abnormal_mean'],
                    'B_separation': summary['metric_B']['separation'],
                    'B_overlap': summary['metric_B']['overlap_ratio'],
                    # Metric C: åŸå‹å¡Œç¼©
                    'C_collapse_score': summary['metric_C']['collapse_score'],
                    'C_similarity_mean': summary['metric_C']['mean'],
                    # Metric D: ååŸå‹å½’å› 
                    'D_max_proto_count': max(summary['metric_D']['proto_counts']),
                    'D_high_score_max': max(summary['metric_D']['high_score_proto_counts']),
                    # Metric E: èåˆæ•æ„Ÿæ€§
                    'E_normal_semantic': summary['metric_E']['normal_semantic_mean'],
                    'E_abnormal_semantic': summary['metric_E']['abnormal_semantic_mean'],
                }
                all_metrics.append(metrics)
        else:
            missing_classes.append(class_name)
    
    # è½¬ä¸ºDataFrame
    metrics_df = pd.DataFrame(all_metrics)
    
    # åˆå¹¶æ€§èƒ½æ•°æ®
    full_data = performance_data.merge(metrics_df, on='class', how='left')
    
    # ä¿å­˜å®Œæ•´æ•°æ®
    full_data.to_csv('analysis/full_metrics_k2.csv', index=False)
    
    print(f"âœ… æˆåŠŸæ±‡æ€»: {len(all_metrics)}/{len(classes_to_diagnose)} ä¸ªç±»åˆ«")
    if missing_classes:
        print(f"âš ï¸  ç¼ºå¤±è¯Šæ–­æ•°æ®: {len(missing_classes)} ä¸ªç±»åˆ«")
        for cls in missing_classes:
            print(f"    - {cls}")
    
    print()
    print("="*80)
    print(f"âœ… å®Œæ•´æŒ‡æ ‡çŸ©é˜µå·²ä¿å­˜åˆ°: analysis/full_metrics_k2.csv")
    print("="*80)
    print()
    
    # æ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡
    print("ğŸ“Š æŒ‡æ ‡æ±‡æ€»ç»Ÿè®¡:")
    print("-"*80)
    
    metric_summary = {
        'A_hit_mean': 'å¼‚å¸¸maxå¶ç„¶å‘½ä¸­(å‡å€¼)',
        'A_hit_p95': 'å¼‚å¸¸maxå¶ç„¶å‘½ä¸­(P95)',
        'B_separation': 'åˆ¤åˆ«åˆ†ç¦»åº¦',
        'B_overlap': 'è£•åº¦é‡å ç‡',
        'C_collapse_score': 'åŸå‹å¡Œç¼©åˆ†æ•°',
    }
    
    for metric, label in metric_summary.items():
        if metric in full_data.columns:
            mean_val = full_data[metric].mean()
            std_val = full_data[metric].std()
            min_val = full_data[metric].min()
            max_val = full_data[metric].max()
            print(f"{label:<25} å‡å€¼={mean_val:.4f}, æ ‡å‡†å·®={std_val:.4f}, èŒƒå›´=[{min_val:.4f}, {max_val:.4f}]")
    
    print()
    print("="*80)
    print("âœ… Step 2 å®Œæˆï¼")
    print("="*80)

if __name__ == '__main__':
    main()
