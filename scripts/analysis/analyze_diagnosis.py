#!/usr/bin/env python3
"""
åˆ†æè¯Šæ–­ç»“æœï¼Œå¯¹æ¯”é€€åŒ–ç±» vs æ”¹è¿›ç±»
"""

import os
import json
import pandas as pd
import numpy as np

# å®šä¹‰ç±»åˆ«ç±»å‹
degraded_classes = {
    'mvtec-toothbrush': {'baseline': 98.89, 'prompt2': 86.94, 'delta': -11.95},
    'mvtec-capsule': {'baseline': 79.94, 'prompt2': 69.12, 'delta': -10.82},
    'mvtec-cable': {'baseline': 97.42, 'prompt2': 96.08, 'delta': -1.34},
    'visa-pcb2': {'baseline': 98.20, 'prompt2': 93.54, 'delta': -4.66},
    'visa-pipe_fryum': {'baseline': 97.37, 'prompt2': 88.84, 'delta': -8.53}
}

improved_classes = {
    'mvtec-screw': {'baseline': 58.66, 'prompt2': 73.36, 'delta': +14.70}
}

def load_class_summary(class_name):
    """åŠ è½½æŸä¸ªç±»åˆ«çš„è¯Šæ–­æ‘˜è¦"""
    dataset = class_name.split('-')[0]
    class_short = class_name.split('-')[1]
    
    summary_path = f'diagnostics/{dataset}_k2_{class_short}/summary.json'
    if not os.path.exists(summary_path):
        return None
    
    with open(summary_path, 'r') as f:
        return json.load(f)

def main():
    print("="*80)
    print("å¤šåŸå‹é€€åŒ–è¯Šæ–­åˆ†ææŠ¥å‘Š")
    print("="*80)
    print()
    
    # æ”¶é›†æ‰€æœ‰ç±»åˆ«çš„æŒ‡æ ‡
    all_metrics = []
    
    print("1ï¸âƒ£  é€€åŒ–ç±»åˆ« (Degraded Classes)")
    print("-"*80)
    for class_name, perf in degraded_classes.items():
        summary = load_class_summary(class_name)
        if summary is None:
            print(f"âš ï¸  {class_name}: æœªæ‰¾åˆ°è¯Šæ–­æ•°æ®")
            continue
        
        metrics = {
            'class': class_name,
            'type': 'é€€åŒ–ç±»',
            'baseline': perf['baseline'],
            'prompt2': perf['prompt2'],
            'delta': perf['delta'],
            'A_ab_max_mean': summary['metric_A']['mean'],
            'A_ab_max_p95': summary['metric_A']['p95'],
            'A_hit_rate_30': summary['metric_A']['hit_rate_0.3'],
            'B_normal_margin': summary['metric_B']['normal_mean'],
            'B_abnormal_margin': summary['metric_B']['abnormal_mean'],
            'B_separation': summary['metric_B']['separation'],
            'B_overlap': summary['metric_B']['overlap_ratio'],
            'C_collapse_score': summary['metric_C']['collapse_score'],
            'D_top1_count': summary['metric_D']['proto_counts'][np.argmax(summary['metric_D']['proto_counts'])]
        }
        all_metrics.append(metrics)
        
        print(f"\nğŸ“Š {class_name} (Baseline: {perf['baseline']:.2f}% â†’ Prompt2: {perf['prompt2']:.2f}%, Î”={perf['delta']:.2f}%)")
        print(f"   [A] å¼‚å¸¸maxå¶ç„¶å‘½ä¸­: å‡å€¼={metrics['A_ab_max_mean']:.3f}, P95={metrics['A_ab_max_p95']:.3f}, >0.3å‘½ä¸­ç‡={metrics['A_hit_rate_30']:.2%}")
        print(f"   [B] åˆ¤åˆ«è£•åº¦: Normal={metrics['B_normal_margin']:.4f}, Abnormal={metrics['B_abnormal_margin']:.4f}, åˆ†ç¦»åº¦={metrics['B_separation']:.4f}, é‡å ={metrics['B_overlap']:.2%}")
        print(f"   [C] åŸå‹å¡Œç¼©: collapse_score={metrics['C_collapse_score']:.3f}")
        print(f"   [D] ååŸå‹: Top1åŸå‹å‘½ä¸­{metrics['D_top1_count']}æ¬¡")
    
    print()
    print("2ï¸âƒ£  æ”¹è¿›ç±»åˆ« (Improved Classes)")
    print("-"*80)
    for class_name, perf in improved_classes.items():
        summary = load_class_summary(class_name)
        if summary is None:
            print(f"âš ï¸  {class_name}: æœªæ‰¾åˆ°è¯Šæ–­æ•°æ®")
            continue
        
        metrics = {
            'class': class_name,
            'type': 'æ”¹è¿›ç±»',
            'baseline': perf['baseline'],
            'prompt2': perf['prompt2'],
            'delta': perf['delta'],
            'A_ab_max_mean': summary['metric_A']['mean'],
            'A_ab_max_p95': summary['metric_A']['p95'],
            'A_hit_rate_30': summary['metric_A']['hit_rate_0.3'],
            'B_normal_margin': summary['metric_B']['normal_mean'],
            'B_abnormal_margin': summary['metric_B']['abnormal_mean'],
            'B_separation': summary['metric_B']['separation'],
            'B_overlap': summary['metric_B']['overlap_ratio'],
            'C_collapse_score': summary['metric_C']['collapse_score'],
            'D_top1_count': summary['metric_D']['proto_counts'][np.argmax(summary['metric_D']['proto_counts'])]
        }
        all_metrics.append(metrics)
        
        print(f"\nğŸ“Š {class_name} (Baseline: {perf['baseline']:.2f}% â†’ Prompt2: {perf['prompt2']:.2f}%, Î”={perf['delta']:.2f}%)")
        print(f"   [A] å¼‚å¸¸maxå¶ç„¶å‘½ä¸­: å‡å€¼={metrics['A_ab_max_mean']:.3f}, P95={metrics['A_ab_max_p95']:.3f}, >0.3å‘½ä¸­ç‡={metrics['A_hit_rate_30']:.2%}")
        print(f"   [B] åˆ¤åˆ«è£•åº¦: Normal={metrics['B_normal_margin']:.4f}, Abnormal={metrics['B_abnormal_margin']:.4f}, åˆ†ç¦»åº¦={metrics['B_separation']:.4f}, é‡å ={metrics['B_overlap']:.2%}")
        print(f"   [C] åŸå‹å¡Œç¼©: collapse_score={metrics['C_collapse_score']:.3f}")
        print(f"   [D] ååŸå‹: Top1åŸå‹å‘½ä¸­{metrics['D_top1_count']}æ¬¡")
    
    # ç»Ÿè®¡å¯¹æ¯”
    print()
    print("="*80)
    print("3ï¸âƒ£  é€€åŒ–ç±» vs æ”¹è¿›ç±» å¯¹æ¯”ç»Ÿè®¡")
    print("="*80)
    
    df = pd.DataFrame(all_metrics)
    
    # æŒ‰ç±»å‹åˆ†ç»„
    degraded_df = df[df['type'] == 'é€€åŒ–ç±»']
    improved_df = df[df['type'] == 'æ”¹è¿›ç±»']
    
    print()
    print("ğŸ“ˆ æŒ‡æ ‡å‡å€¼å¯¹æ¯”:")
    print("-"*80)
    print(f"{'æŒ‡æ ‡':<30} {'é€€åŒ–ç±»å‡å€¼':>15} {'æ”¹è¿›ç±»å‡å€¼':>15} {'å·®å¼‚':>15}")
    print("-"*80)
    
    metrics_to_compare = [
        ('A_ab_max_mean', 'å¼‚å¸¸maxå¶ç„¶å‘½ä¸­(å‡å€¼)'),
        ('A_ab_max_p95', 'å¼‚å¸¸maxå¶ç„¶å‘½ä¸­(P95)'),
        ('A_hit_rate_30', 'å¶ç„¶å‘½ä¸­ç‡>0.3'),
        ('B_normal_margin', 'Normalæ ·æœ¬è£•åº¦'),
        ('B_separation', 'åˆ¤åˆ«åˆ†ç¦»åº¦'),
        ('B_overlap', 'è£•åº¦é‡å ç‡'),
        ('C_collapse_score', 'åŸå‹å¡Œç¼©åˆ†æ•°'),
    ]
    
    for metric_name, metric_label in metrics_to_compare:
        degraded_mean = degraded_df[metric_name].mean()
        improved_mean = improved_df[metric_name].mean()
        diff = degraded_mean - improved_mean
        
        # åˆ¤æ–­è¶‹åŠ¿
        if metric_name in ['A_ab_max_mean', 'A_ab_max_p95', 'A_hit_rate_30', 'B_overlap', 'C_collapse_score']:
            # è¿™äº›æŒ‡æ ‡è¶Šé«˜è¶Šå
            trend = "â¬†ï¸ é€€åŒ–ç±»æ›´é«˜" if diff > 0 else "â¬‡ï¸ æ”¹è¿›ç±»æ›´é«˜"
        else:
            # è¿™äº›æŒ‡æ ‡è¶Šé«˜è¶Šå¥½
            trend = "â¬†ï¸ é€€åŒ–ç±»æ›´å¥½" if diff > 0 else "â¬‡ï¸ æ”¹è¿›ç±»æ›´å¥½"
        
        print(f"{metric_label:<30} {degraded_mean:>15.4f} {improved_mean:>15.4f} {diff:>15.4f}  {trend}")
    
    # å…³é”®å‘ç°
    print()
    print("="*80)
    print("4ï¸âƒ£  å…³é”®å‘ç° (Key Findings)")
    print("="*80)
    
    findings = []
    
    # å‡è®¾1: å¼‚å¸¸maxå¶ç„¶å‘½ä¸­
    degraded_hit_mean = degraded_df['A_ab_max_mean'].mean()
    improved_hit_mean = improved_df['A_ab_max_mean'].mean()
    if degraded_hit_mean > improved_hit_mean:
        findings.append({
            'hypothesis': 'å‡è®¾1: å¼‚å¸¸maxå¶ç„¶å‘½ä¸­',
            'evidence': f'é€€åŒ–ç±»å¼‚å¸¸maxå‡å€¼({degraded_hit_mean:.3f}) > æ”¹è¿›ç±»({improved_hit_mean:.3f})',
            'conclusion': 'âœ… æ”¯æŒ - é€€åŒ–ç±»ç¡®å®æœ‰æ›´é«˜çš„å¶ç„¶å‘½ä¸­',
            'severity': 'HIGH' if degraded_hit_mean > 0.25 else 'MEDIUM'
        })
    else:
        findings.append({
            'hypothesis': 'å‡è®¾1: å¼‚å¸¸maxå¶ç„¶å‘½ä¸­',
            'evidence': f'é€€åŒ–ç±»å¼‚å¸¸maxå‡å€¼({degraded_hit_mean:.3f}) â‰¤ æ”¹è¿›ç±»({improved_hit_mean:.3f})',
            'conclusion': 'âŒ ä¸æ”¯æŒ',
            'severity': 'LOW'
        })
    
    # å‡è®¾2: åŸå‹å¡Œç¼©
    degraded_collapse = degraded_df['C_collapse_score'].mean()
    improved_collapse = improved_df['C_collapse_score'].mean()
    if degraded_collapse > improved_collapse:
        findings.append({
            'hypothesis': 'å‡è®¾2: åŸå‹å¡Œç¼©',
            'evidence': f'é€€åŒ–ç±»å¡Œç¼©åˆ†æ•°({degraded_collapse:.3f}) > æ”¹è¿›ç±»({improved_collapse:.3f})',
            'conclusion': 'âœ… æ”¯æŒ - é€€åŒ–ç±»åŸå‹æ›´å†—ä½™',
            'severity': 'HIGH' if degraded_collapse > 0.93 else 'MEDIUM'
        })
    else:
        findings.append({
            'hypothesis': 'å‡è®¾2: åŸå‹å¡Œç¼©',
            'evidence': f'é€€åŒ–ç±»å¡Œç¼©åˆ†æ•°({degraded_collapse:.3f}) â‰¤ æ”¹è¿›ç±»({improved_collapse:.3f})',
            'conclusion': 'âŒ ä¸æ”¯æŒ',
            'severity': 'LOW'
        })
    
    # å‡è®¾3: åˆ¤åˆ«è£•åº¦ä¸è¶³
    degraded_sep = degraded_df['B_separation'].mean()
    improved_sep = improved_df['B_separation'].mean()
    if degraded_sep < improved_sep:
        findings.append({
            'hypothesis': 'å‡è®¾3: åˆ¤åˆ«è£•åº¦ä¸è¶³',
            'evidence': f'é€€åŒ–ç±»åˆ†ç¦»åº¦({degraded_sep:.4f}) < æ”¹è¿›ç±»({improved_sep:.4f})',
            'conclusion': 'âœ… æ”¯æŒ - é€€åŒ–ç±»åˆ¤åˆ«èƒ½åŠ›æ›´å¼±',
            'severity': 'HIGH' if abs(degraded_sep) < 0.005 else 'MEDIUM'
        })
    else:
        findings.append({
            'hypothesis': 'å‡è®¾3: åˆ¤åˆ«è£•åº¦ä¸è¶³',
            'evidence': f'é€€åŒ–ç±»åˆ†ç¦»åº¦({degraded_sep:.4f}) â‰¥ æ”¹è¿›ç±»({improved_sep:.4f})',
            'conclusion': 'âŒ ä¸æ”¯æŒ',
            'severity': 'LOW'
        })
    
    # æ‰“å°å‘ç°
    for i, finding in enumerate(findings, 1):
        print(f"\n{i}. {finding['hypothesis']}")
        print(f"   è¯æ®: {finding['evidence']}")
        print(f"   ç»“è®º: {finding['conclusion']}")
        print(f"   ä¸¥é‡æ€§: {finding['severity']}")
    
    # ä¿å­˜åˆ†æç»“æœ
    df.to_csv('diagnostics/analysis_summary.csv', index=False)
    print()
    print("="*80)
    print("âœ… åˆ†æå®Œæˆï¼è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: diagnostics/analysis_summary.csv")
    print("="*80)

if __name__ == '__main__':
    main()
